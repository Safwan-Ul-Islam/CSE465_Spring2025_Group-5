{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdgpCTHa8_US"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Constants\n",
        "INPUT_SHAPE = (256, 256, 1)\n",
        "GITHUB_REPO = \"https://github.com/Safwan-Ul-Islam/CSE465_Spring2025_Group-5/tree/main\"  # Replace with your GitHub repo\n",
        "MODEL_URL = f\"{GITHUB_REPO}/raw/main/Final%20project/best_model.h5\"  # Path to model in repo\n",
        "TEST_DATA_URL = f\"{GITHUB_REPO}/archive/main.zip\"  # Will download entire repo as zip\n",
        "LOCAL_DIR = \"./test_data\"\n",
        "TEST_IMAGE_DIR = os.path.join(LOCAL_DIR, \"Final project/dataset/test_set/images\")  # Adjust paths based on your repo structure\n",
        "TEST_MASK_DIR = os.path.join(LOCAL_DIR, \"Final project/dataset/dataset/test_set/masks\")  # Adjust paths based on your repo structure\n",
        "NUM_SAMPLES_TO_VISUALIZE = 5\n",
        "\n",
        "# Download helper function\n",
        "def download_from_github(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download from {url}\")\n",
        "\n",
        "# Download and extract dataset\n",
        "def setup_test_data():\n",
        "    if not os.path.exists(LOCAL_DIR):\n",
        "        os.makedirs(LOCAL_DIR)\n",
        "\n",
        "    # Download the zip file\n",
        "    print(\"Downloading test data from GitHub...\")\n",
        "    response = requests.get(TEST_DATA_URL)\n",
        "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "    zip_file.extractall(LOCAL_DIR)\n",
        "\n",
        "    # The extracted folder might have a different name (like repo-main)\n",
        "    # You might need to adjust paths here based on your actual repo structure\n",
        "    print(\"Test data downloaded and extracted.\")\n",
        "\n",
        "# Metrics (same as before)\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def iou_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
        "    return K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred_thresh = (y_pred > 0.5).astype(np.float32)\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred_thresh.flatten()\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true_f, y_pred_f).ravel()\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    dice = (2 * tp) / (2 * tp + fp + fn)\n",
        "    iou = tp / (tp + fp + fn)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'sensitivity': sensitivity,\n",
        "        'specificity': specificity,\n",
        "        'dice': dice,\n",
        "        'iou': iou\n",
        "    }\n",
        "\n",
        "# Load and preprocess test data (same as before)\n",
        "def load_and_preprocess_test_data(image_dir, mask_dir):\n",
        "    image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')])\n",
        "    mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
        "\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, (INPUT_SHAPE[0], INPUT_SHAPE[1]), interpolation=cv2.INTER_NEAREST)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        mask = mask.astype(np.float32) / 255.0\n",
        "        mask = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "        images.append(img)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(mask_paths), np.array(masks)\n",
        "\n",
        "# Visualize predictions (same as before)\n",
        "def visualize_predictions(images, true_masks, pred_masks, num_samples=5):\n",
        "    plt.figure(figsize=(15, 5*num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(num_samples, 3, i*3 + 1)\n",
        "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_samples, 3, i*3 + 2)\n",
        "        plt.imshow(true_masks[i].squeeze(), cmap='gray')\n",
        "        plt.title('True Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_samples, 3, i*3 + 3)\n",
        "        plt.imshow(pred_masks[i].squeeze(), cmap='gray')\n",
        "        plt.title('Predicted Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main test function\n",
        "def test_model():\n",
        "    # Download model\n",
        "    print(\"Downloading model from GitHub...\")\n",
        "    model_path = os.path.join(LOCAL_DIR, \"best_model.h5\")\n",
        "    download_from_github(MODEL_URL, model_path)\n",
        "\n",
        "    # Download and setup test data\n",
        "    setup_test_data()\n",
        "\n",
        "    # Load the trained model\n",
        "    model = tf.keras.models.load_model(model_path, custom_objects={\n",
        "        'dice_coef': dice_coef,\n",
        "        'iou_coef': iou_coef,\n",
        "        'dice_loss': lambda y_true, y_pred: 1 - dice_coef(y_true, y_pred),\n",
        "        'bce_dice_loss': lambda y_true, y_pred: tf.keras.losses.binary_crossentropy(y_true, y_pred) + (1 - dice_coef(y_true, y_pred))\n",
        "    })\n",
        "\n",
        "    # Load test data\n",
        "    test_images, test_mask_paths, test_masks = load_and_preprocess_test_data(TEST_IMAGE_DIR, TEST_MASK_DIR)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(test_images)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(test_masks, predictions)\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Sensitivity (Recall): {metrics['sensitivity']:.4f}\")\n",
        "    print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
        "    print(f\"Dice Coefficient: {metrics['dice']:.4f}\")\n",
        "    print(f\"IoU (Jaccard Index): {metrics['iou']:.4f}\")\n",
        "\n",
        "    # Visualize some predictions\n",
        "    print(f\"\\nVisualizing {NUM_SAMPLES_TO_VISUALIZE} predictions...\")\n",
        "    visualize_predictions(\n",
        "        test_images[:NUM_SAMPLES_TO_VISUALIZE],\n",
        "        test_masks[:NUM_SAMPLES_TO_VISUALIZE],\n",
        "        (predictions[:NUM_SAMPLES_TO_VISUALIZE] > 0.5).astype(np.float32)\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qwf4ZddCBrsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wYP9DypkBsIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "from skimage.measure import label, regionprops\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define custom metrics\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "# GitHub configuration\n",
        "GITHUB_REPO = \"https://github.com/Safwan-Ul-Islam/CSE465_Spring2025_Group-5/tree/main\"  # Replace with your GitHub repo\n",
        "MODEL_URL = f\"{GITHUB_REPO}/raw/main/Final%20project/best_model.h5\"  # Path to model in repo\n",
        "CSV_URL = f\"{GITHUB_REPO}/raw/main/Final%20projectdataset/pixel_size_and_HC.csv\"  # Path to CSV in repo\n",
        "TEST_DATA_URL = f\"{GITHUB_REPO}/archive/main.zip\"  # Will download entire repo as zip\n",
        "\n",
        "# Local paths\n",
        "LOCAL_DIR = \"./github_data\"\n",
        "MODEL_PATH = os.path.join(LOCAL_DIR, \"Final project/best_model.h5\")\n",
        "TEST_IMAGE_DIR = os.path.join(LOCAL_DIR, \"Final project/dataset/dataset/test_set/images\")  # Adjust based on your repo structure\n",
        "CSV_PATH = os.path.join(LOCAL_DIR, \"Final project/dataset/pixel_size_and_HC.csv\")\n",
        "OUTPUT_DIR = os.path.join(LOCAL_DIR, \"Final project/results\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Download helper function\n",
        "def download_from_github(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download from {url}\")\n",
        "\n",
        "# Download and extract dataset\n",
        "def setup_test_data():\n",
        "    if not os.path.exists(LOCAL_DIR):\n",
        "        os.makedirs(LOCAL_DIR)\n",
        "\n",
        "    # Download the model\n",
        "    print(\"Downloading model from GitHub...\")\n",
        "    download_from_github(MODEL_URL, MODEL_PATH)\n",
        "\n",
        "    # Download the CSV\n",
        "    print(\"Downloading CSV from GitHub...\")\n",
        "    download_from_github(CSV_URL, CSV_PATH)\n",
        "\n",
        "    # Download the test images (entire repo as zip)\n",
        "    print(\"Downloading test data from GitHub...\")\n",
        "    response = requests.get(TEST_DATA_URL)\n",
        "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "    zip_file.extractall(LOCAL_DIR)\n",
        "\n",
        "    print(\"All data downloaded and extracted.\")\n",
        "\n",
        "# Function to calculate head circumference from mask\n",
        "def calculate_hc_from_mask(mask, pixel_size):\n",
        "    labeled_mask = label(mask)\n",
        "    regions = regionprops(labeled_mask)\n",
        "\n",
        "    if len(regions) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    largest_region = max(regions, key=lambda x: x.area)\n",
        "    perimeter = largest_region.perimeter\n",
        "    circumference_mm = perimeter * pixel_size\n",
        "\n",
        "    return circumference_mm\n",
        "\n",
        "# Main processing function\n",
        "def process_hc_measurements():\n",
        "    # Setup data from GitHub\n",
        "    setup_test_data()\n",
        "\n",
        "    # Load the trained model with custom objects\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(\n",
        "            MODEL_PATH,\n",
        "            custom_objects={\n",
        "                'dice_coef': dice_coef,\n",
        "                'dice_loss': dice_loss,\n",
        "                'bce_dice_loss': bce_dice_loss\n",
        "            }\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "    # Load the CSV with ground truth measurements\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df['filename'] = df['filename'].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "    # Process test images and compare with ground truth\n",
        "    results = []\n",
        "    for filename in os.listdir(TEST_IMAGE_DIR):\n",
        "        if filename.endswith('.png'):\n",
        "            csv_row = df[df['filename'] == filename]\n",
        "            if len(csv_row) == 0:\n",
        "                print(f\"No CSV entry found for {filename}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            pixel_size = csv_row['pixel size(mm)'].values[0]\n",
        "            true_hc = csv_row['head circumference (mm)'].values[0]\n",
        "\n",
        "            img_path = os.path.join(TEST_IMAGE_DIR, filename)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if img is None:\n",
        "                print(f\"Could not read image {filename}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, (256, 256))\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "\n",
        "            try:\n",
        "                pred_mask = model.predict(img, verbose=0)[0]\n",
        "                pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "                pred_hc = calculate_hc_from_mask(pred_mask.squeeze(), pixel_size)\n",
        "\n",
        "                error_mm = abs(pred_hc - true_hc)\n",
        "                error_percent = (error_mm / true_hc) * 100 if true_hc != 0 else 0\n",
        "\n",
        "                results.append({\n",
        "                    'filename': filename,\n",
        "                    'true_hc': true_hc,\n",
        "                    'pred_hc': pred_hc,\n",
        "                    'error_mm': error_mm,\n",
        "                    'error_percent': error_percent,\n",
        "                    'pixel_size': pixel_size\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    if results:\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Calculate overall accuracy metrics\n",
        "        mean_error_mm = results_df['error_mm'].mean()\n",
        "        mean_error_percent = results_df['error_percent'].mean()\n",
        "        std_error_mm = results_df['error_mm'].std()\n",
        "        median_error_mm = results_df['error_mm'].median()\n",
        "\n",
        "        print(\"\\nHead Circumference Measurement Results:\")\n",
        "        print(f\"Number of samples analyzed: {len(results_df)}\")\n",
        "        print(f\"Mean Absolute Error: {mean_error_mm:.2f} mm\")\n",
        "        print(f\"Median Absolute Error: {median_error_mm:.2f} mm\")\n",
        "        print(f\"Mean Percentage Error: {mean_error_percent:.2f}%\")\n",
        "        print(f\"Standard Deviation: {std_error_mm:.2f} mm\")\n",
        "\n",
        "        # Save results to CSV\n",
        "        results_csv_path = os.path.join(OUTPUT_DIR, 'hc_measurement_results.csv')\n",
        "        results_df.to_csv(results_csv_path, index=False)\n",
        "        print(f\"\\nResults saved to: {results_csv_path}\")\n",
        "\n",
        "        # Visualize some examples with measurements\n",
        "        num_samples = min(5, len(results_df))\n",
        "        plt.figure(figsize=(15, 3*num_samples))\n",
        "\n",
        "        for i, (_, row) in enumerate(results_df.head(num_samples).iterrows()):\n",
        "            img_path = os.path.join(TEST_IMAGE_DIR, row['filename'])\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            img_processed = cv2.resize(img, (256, 256))\n",
        "            img_processed = np.expand_dims(img_processed, axis=-1)\n",
        "            img_processed = img_processed.astype(np.float32) / 255.0\n",
        "            img_processed = np.expand_dims(img_processed, axis=0)\n",
        "            pred_mask = model.predict(img_processed, verbose=0)[0]\n",
        "            pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "            plt.subplot(num_samples, 2, 2*i+1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Original: {row['filename']}\\nTrue HC: {row['true_hc']:.1f}mm\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 2, 2*i+2)\n",
        "            plt.imshow(pred_mask.squeeze(), cmap='gray')\n",
        "            plt.title(f\"Predicted HC: {row['pred_hc']:.1f}mm\\nError: {row['error_mm']:.1f}mm ({row['error_percent']:.1f}%)\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'hc_comparison.png'))\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid results were generated. Please check your input paths and data.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_hc_measurements()"
      ],
      "metadata": {
        "id": "MFUqxdmXBrWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "from skimage.measure import label, regionprops\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "# GitHub configuration\n",
        "GITHUB_REPO = \"https://github.com/Safwan-Ul-Islam/CSE465_Spring2025_Group-5/tree/main\"  # Replace with your GitHub repo\n",
        "MODEL_URL = f\"{GITHUB_REPO}/raw/main/Final%20project/best_model.h5\"  # Path to model in repo\n",
        "CSV_URL = f\"{GITHUB_REPO}/raw/main/Final%20project/dataset/pixel_size_and_HC.csv\"  # Path to CSV in repo\n",
        "TEST_DATA_URL = f\"{GITHUB_REPO}/archive/main.zip\"  # Will download entire repo as zip\n",
        "\n",
        "# Local paths\n",
        "LOCAL_DIR = \"./github_data\"\n",
        "MODEL_PATH = os.path.join(LOCAL_DIR, \"Final project/best_model.h5\")\n",
        "TEST_IMAGE_DIR = os.path.join(LOCAL_DIR, \"Final project/dataset/test_set/images\")  # Adjust based on your repo structure\n",
        "CSV_PATH = os.path.join(LOCAL_DIR, \"Final project/dataset/pixel_size_and_HC.csv\")\n",
        "OUTPUT_DIR = os.path.join(LOCAL_DIR, \"Final project/results\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define all custom objects that might be in the model\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def download_from_github(url, save_path):\n",
        "    \"\"\"Download a file from GitHub\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download from {url}\")\n",
        "\n",
        "def setup_data():\n",
        "    \"\"\"Download all required data from GitHub\"\"\"\n",
        "    if not os.path.exists(LOCAL_DIR):\n",
        "        os.makedirs(LOCAL_DIR)\n",
        "\n",
        "    # Download model\n",
        "    print(\"Downloading model...\")\n",
        "    download_from_github(MODEL_URL, MODEL_PATH)\n",
        "\n",
        "    # Download CSV\n",
        "    print(\"Downloading CSV...\")\n",
        "    download_from_github(CSV_URL, CSV_PATH)\n",
        "\n",
        "    # Download test images (entire repo as zip)\n",
        "    print(\"Downloading test images...\")\n",
        "    response = requests.get(TEST_DATA_URL)\n",
        "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "    zip_file.extractall(LOCAL_DIR)\n",
        "\n",
        "    print(\"All data downloaded successfully\")\n",
        "\n",
        "def calculate_hc(mask, pixel_size):\n",
        "    \"\"\"Calculate head circumference from binary mask\"\"\"\n",
        "    mask = mask.squeeze().astype(np.uint8)\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not contours:\n",
        "        return 0.0\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    perimeter = cv2.arcLength(largest_contour, True)\n",
        "    return perimeter * pixel_size\n",
        "\n",
        "def main():\n",
        "    # Setup data from GitHub\n",
        "    setup_data()\n",
        "\n",
        "    # Load model with multiple fallback options\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(\n",
        "            MODEL_PATH,\n",
        "            custom_objects={\n",
        "                'dice_coef': dice_coef,\n",
        "                'dice_loss': dice_loss,\n",
        "                'bce_dice_loss': bce_dice_loss\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        try:\n",
        "            model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
        "            model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load model: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Load ground truth data\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df['filename'] = df['filename'].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "    # Process all test images\n",
        "    results = []\n",
        "    for filename in os.listdir(TEST_IMAGE_DIR):\n",
        "        if not filename.endswith('.png'):\n",
        "            continue\n",
        "\n",
        "        # Find matching CSV entry\n",
        "        csv_match = df[df['filename'] == filename]\n",
        "        if csv_match.empty:\n",
        "            print(f\"No CSV match for {filename}\")\n",
        "            continue\n",
        "\n",
        "        pixel_size = csv_match['pixel size(mm)'].values[0]\n",
        "        true_hc = csv_match['head circumference (mm)'].values[0]\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img_path = os.path.join(TEST_IMAGE_DIR, filename)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            print(f\"Failed to load {filename}\")\n",
        "            continue\n",
        "\n",
        "        img_proc = cv2.resize(img, (256, 256))\n",
        "        img_proc = np.expand_dims(img_proc, axis=-1)\n",
        "        img_proc = img_proc.astype(np.float32) / 255.0\n",
        "\n",
        "        # Predict and calculate HC\n",
        "        pred_mask = model.predict(np.expand_dims(img_proc, axis=0), verbose=0)[0]\n",
        "        pred_hc = calculate_hc((pred_mask > 0.5).astype(np.uint8), pixel_size)\n",
        "\n",
        "        results.append({\n",
        "            'Filename': filename,\n",
        "            'Pixel Size (mm)': f\"{pixel_size:.4f}\",\n",
        "            'True HC (mm)': f\"{true_hc:.2f}\",\n",
        "            'Predicted HC (mm)': f\"{pred_hc:.2f}\",\n",
        "            'Abs Error (mm)': f\"{abs(pred_hc - true_hc):.2f}\",\n",
        "            'Rel Error (%)': f\"{100 * abs(pred_hc - true_hc) / true_hc:.2f}\"\n",
        "        })\n",
        "\n",
        "    # Create and save results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(os.path.join(OUTPUT_DIR, 'hc_comparison_results.csv'), index=False)\n",
        "\n",
        "    # Calculate statistics\n",
        "    stats = {\n",
        "        'Metric': ['Mean', 'Median', 'Std Dev', 'Max', 'Min'],\n",
        "        'Absolute Error (mm)': [\n",
        "            results_df['Abs Error (mm)'].astype(float).mean(),\n",
        "            results_df['Abs Error (mm)'].astype(float).median(),\n",
        "            results_df['Abs Error (mm)'].astype(float).std(),\n",
        "            results_df['Abs Error (mm)'].astype(float).max(),\n",
        "            results_df['Abs Error (mm)'].astype(float).min()\n",
        "        ],\n",
        "        'Relative Error (%)': [\n",
        "            results_df['Rel Error (%)'].astype(float).mean(),\n",
        "            results_df['Rel Error (%)'].astype(float).median(),\n",
        "            results_df['Rel Error (%)'].astype(float).std(),\n",
        "            results_df['Rel Error (%)'].astype(float).max(),\n",
        "            results_df['Rel Error (%)'].astype(float).min()\n",
        "        ]\n",
        "    }\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "    stats_df.to_csv(os.path.join(OUTPUT_DIR, 'hc_error_statistics.csv'), index=False)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n=== HC MEASUREMENT COMPARISON ===\")\n",
        "    print(tabulate(results_df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    print(\"\\n=== ERROR STATISTICS ===\")\n",
        "    print(tabulate(stats_df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(\n",
        "        results_df['True HC (mm)'].astype(float),\n",
        "        results_df['Predicted HC (mm)'].astype(float),\n",
        "        alpha=0.6\n",
        "    )\n",
        "    plt.plot([0, max(results_df['True HC (mm)'].astype(float))],\n",
        "             [0, max(results_df['True HC (mm)'].astype(float))], 'r--')\n",
        "    plt.xlabel('True HC (mm)')\n",
        "    plt.ylabel('Predicted HC (mm)')\n",
        "    plt.title('True vs Predicted HC')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(results_df['Abs Error (mm)'].astype(float), bins=20, alpha=0.7)\n",
        "    plt.xlabel('Absolute Error (mm)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Error Distribution')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'hc_comparison_plots.png'))\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "G0l42_gKBra9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPK6Bc7WBres"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}